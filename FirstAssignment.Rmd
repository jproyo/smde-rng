<!-- R Commander Markdown Template -->

SMDE - First Assignment
=======================

### Juan Pablo Royo Sales

### `r as.character(Sys.Date())`

```{r echo=FALSE}
# include this code chunk as-is to set options
knitr::opts_chunk$set(comment=NA, prompt=TRUE, out.width=750, fig.height=8, fig.width=8)
library(Rcmdr)
library(car)
library(RcmdrMisc)
```


```{r echo=FALSE}
# include this code chunk as-is to enable 3D graphs
library(rgl)
knitr::knit_hooks$set(webgl = hook_webgl)
```

# Question 1 - Assigment

## Loading data generated by Java FibFlagged method.

1. Load Data

```{r}
fibflagged <- read.table("/Users/juan/Projects/upc/master/smde/smde-rng/500_random_numbers.csv", 
    header = FALSE, sep = ",", na.strings = "NA", dec = ".", strip.white = TRUE)
```

2. Apply bins

```{r}
fibflagged$bins <- with(fibflagged, binVariable(V1, bins = 10, method = "intervals", 
    labels = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10")))
```

3. Generate Frequency

```{r}
fibflagged_trans <- as.data.frame(with(fibflagged, table(bins)))
```


## Generating uniform distribution with R

1. Load data

```{r}
distribution_r <- as.data.frame(matrix(runif(500 * 1, min = 0, max = 1), ncol = 1))
rownames(distribution_r) <- paste("sample", 1:500, sep = "")
colnames(distribution_r) <- "obs"
distribution_r <- within(distribution_r, {
    mean <- rowMeans(distribution_r[, 1:1])
})
```

2. Apply bins

```{r}
distribution_r$bins <- with(distribution_r, binVariable(obs, bins = 10, method = "intervals", 
    labels = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10")))
```

3. Generate Frequency

```{r}
distribution_r_trans <- as.data.frame(with(distribution_r, table(frequency)))
```

## Merge both frequencies R and FibFlagged

```{r}
merge_fibflagged_r_comb <- merge(fibflagged_trans, distribution_r_trans, all = TRUE, 
    by = "row.names")
rownames(merge_fibflagged_r_comb) <- merge_fibflagged_r_comb$Row.names
merge_fibflagged_r_comb$Row.names <- NULL
```

## Generate table for Chi test

```{r}
to_be_test_chi_r_fibflagged <- within(merge_fibflagged_r_comb, {
	bins <- NULL
	frequency <- NULL
})
```

## Run Chi Test

```{r}
test <- chisq.test(to_be_test_chi_r_fibflagged, correct = FALSE)
```

### Conlusion Chi Test

After this **test** has a **p-value** of **0.819475004287325** , therefore with can deduce that we can accept the algorithm but for a number of 10 bins as it seems it is generating Random Numbers in an uniform distribution.


## Another test with 20 bins on each distribution

Now we are going to refrequence the bins to 20

```{r}
fibflagged$morebins <- with(fibflagged, binVariable(V1, bins = 20, method = "intervals", 
    labels = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", 
        "14", "15", "16", "17", "18", "19", "20")))
```


```{r}
distribution_r$morebins <- with(distribution_r, binVariable(obs, bins = 20, method = "intervals", 
    labels = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", 
        "14", "15", "16", "17", "18", "19", "20")))
```

## Count Frequency on 20 bins 

Count the frequency on each distribution

```{r}
fibflagged_trans_20bins <- as.data.frame(with(fibflagged, table(morebins)))
```

```{r}
distribution_r_trans_20bins <- as.data.frame(with(distribution_r, table(morebins)))
```

## Merge both frequency distribution with 20 bins


```{r}
merge_fibflagged_r_comb_20bins <- merge(fibflagged_trans_20bins, distribution_r_trans_20bins, 
    all = TRUE, by = "row.names")
```


```{r}
rownames(merge_fibflagged_r_comb_20bins) <- merge_fibflagged_r_comb_20bins$Row.names
```


```{r}
merge_fibflagged_r_comb_20bins$Row.names <- NULL
```

## Run Chi-square Test


```{r}
test_chi_r_fibflagged_20bins <- within(merge_fibflagged_r_comb_20bins, {
    morebins.x <- NULL
    morebins.y <- NULL
})
```


```{r}
chi_20bins <- chisq.test(test_chi_r_fibflagged_20bins, correct = FALSE)
```

### Conclusion Chi Test

After this **p-value** of **0.00175158290728414**, therefore with can deduce when we increase the beans the algorithm is not generating a random uniform distribution.


# Question 2 - Assignment


## Loading Normal distribution generated by code

1. Normal mu = 0, sigma = 1

```{r}
Norm_m0_s1 <- read.table("/Users/juan/Projects/upc/master/smde/smde-rng/1500_normal_mu_0_sigma_1.csv", 
    header = FALSE, sep = ",", na.strings = "NA", dec = ".", strip.white = TRUE)
```

2. Normal mu = 10, sigma = 1

```{r}
Norm_m10_s1 <- read.table("/Users/juan/Projects/upc/master/smde/smde-rng/1500_normal_mu_10_sigma_1.csv", 
    header = FALSE, sep = ",", na.strings = "NA", dec = ".", strip.white = TRUE)
```

3. Normal mu = 0, sigma = 1

```{r}
Norm_m0_s1_2 <- read.table("/Users/juan/Projects/upc/master/smde/smde-rng/1500_normal_mu_0_sigma_1_2.csv", 
    header = FALSE, sep = ",", na.strings = "NA", dec = ".", strip.white = TRUE)
```


## Run Anova analysis

```{r}
Norm_v1n = data.frame(x1 = Norm_m0_s1, x2 = "v1")
```


```{r}
Norm_v2n = data.frame(x1 = Norm_m10_s1, x2 = "v2")
```


```{r}
Norm_v3n = data.frame(x1 = Norm_m0_s1_2, x2 = "v3")
```


```{r}
data = mergeRows(Norm_v1n, Norm_v2n, common.only = FALSE)
```


```{r}
data = mergeRows(as.data.frame(data), Norm_v3n, common.only = FALSE)
```

```{r}
AnovaModel.1 <- aov(V1 ~ x2, data = data)
```


```{r}
summary(AnovaModel.1)
```


```{r}
Boxplot(V1 ~ x2, data = data, id.method = "y")
```


### Conclusions Anova Analysis 

1. We already know beforehand that we have set 3 different populations which means are different, therefore the small p-value of **Pr(>F) <2e-16** and with a high **F value of 49071** which indicate rejection shows this.
2. For this part of the test we have proof our assumptions.


## Running Test for checking assumptions

```{r}
library("lmtest", lib.loc = "~/R/win-library/3.0")
```


```{r}
dwtest(AnovaModel.1, alternative = "two.sided")
```


```{r}
shapiro.test(residuals(AnovaModel.1))
```


```{r}
lmtest::bptest(AnovaModel.1)
```


### Conclusion Test asssumptions


After running Durbin Watson, Shapiro and Brewusch-Pagan tests we can state the following conclusions:

1. It has been tried different number size of Normal Distribution, first with 500 each, after with 3000 but in this case Shapiro doesn't support such a big number, and finally because of the amount restrictions it has been tested with 1500 samples each Distribution.
2. Regarding Durbin-Watson test, it has been shown that the observations doesnt seem independents at all with a **p-value** of **0.127**
3. In all cases Shapiro Test gave us a really small **p-value** of 0.006966, which it means that the Distribution doesnt seem normal. 
4. Finally for Homogeneity of variance with the last test it has been found out that **p-value** is **1** which means it is homogeneous. 

It can be deduced based on Question 1, that the RNG it is not so random, because in Question 1 when we increase the bins, we obtained a rejection area.

## Anova on Wine FactoMinerR Dataset

First we load the Wine Dataset

```{r}
wine_data <- data(wine, package = "FactoMineR")
```

### Analysis of Soil regarding Odor Intensity before shaking

```{r}
wine_data_soil_odori <- wine[, c("Odor.Intensity.before.shaking", "Soil")]
colnames(wine_data_soil_odori) <- c("odori", "soil")
AnovaModel_soil_odori <- aov(odori ~ soil, data = wine_data_soil_odori)
summary(AnovaModel_soil_odori)
```

Running the previous Annova Model on **Soil** taking the **factor of Odor Intensity before shaking** we obtain a small p-value of **Pr(>F) 0.00397** and with a **F value of 6.496** indicating that we can reject the null hipotesis, therefore the populations are different.
This means that different Soils are affecting the Odor intensity before shaking factor.



### Analysis of Soil regarding Aroma Quality before shaking


```{r}
wine_data_soil_aromaq <- wine[, c("Aroma.quality.before.shaking", "Soil")]
colnames(wine_data_soil_aromaq) <- c("aromaq", "soil")
AnovaModel_soil_aromaq <- aov(aromaq ~ soil, data = wine_data_soil_aromaq)
summary(AnovaModel_soil_aromaq)
```

Running the previous Annova Model on **Soil** taking the **factor of Aroma Quality before shaking** we obtain a small p-value of **Pr(>F) 0.0248** and with a **F value of 4.023** indicating that we can reject the null hipotesis, therefore the populations are different.
This means that different Soils are affecting the Aroma quality of the wine before shaking.


### Analysis of Label regarding Odor Intensity before shaking

```{r}
wine_data_label_odori <- wine[, c("Odor.Intensity.before.shaking", "Label")]
colnames(wine_data_label_odori) <- c("odori", "label")
AnovaModel_label_odori <- aov(odori ~ label, data = wine_data_label_odori)
summary(AnovaModel_label_odori)
```

Running the previous Annova Model on **Label** taking the **factor of Odor Intensity before shaking** we obtain a small p-value of **Pr(>F) 0.0231** and with a **F value of 4.679** indicating that we can reject the null hipotesis, therefore the populations are different.
This means that different Labels are also affecting the Odor intensity before shaking factor.



### Analysis of Label regarding Aroma Quality before shaking


```{r}
wine_data_label_aromaq <- wine[, c("Aroma.quality.before.shaking", "Label")]
colnames(wine_data_label_aromaq) <- c("aromaq", "label")
AnovaModel_label_aromaq <- aov(aromaq ~ label, data = wine_data_label_aromaq)
summary(AnovaModel_label_aromaq)
```


Running the previous Annova Model on **Label** taking the **factor of Aroma Quality before shaking** we obtain a small p-value of **Pr(>F) 0.187** and with a **F value of 1.843** indicating that we can reject the null hipotesis, therefore the populations are different.
This means that different Labels are also affecting the Aroma quality of the wine before shaking.


### Conclusions ANOVA on wine FactoMineR

It seems that both are features Label and Soil are afecting wine factors. Although this, we can state from the analysis above that in the case of **Label** feature for Aroma Quality before shaking,
it is not affecting so much as the other because in spite the *p-value* is small it is near 0.20 which indicates that the populations are more similar in means than the other cases.


# Question 3 - Assigment

## Decathlon Data

### Loading data

```{r}
data(decathlon, package = "FactoMineR")
names(decathlon) <- make.names(names(decathlon))
```

### Spliting Data - Training and Test

```{r}
if (!require("caTools")) {
    install.packages("caTools")
    library(caTools)
}
split = sample.split(decathlon, SplitRatio = 0.6)
training_decathlon_set = subset(decathlon, split == TRUE)
test_decathlon_set = subset(decathlon, split == FALSE)
```

## Analysing Data

### First Scenario of 1500m with all Variables

```{r}
names(training_decathlon_set) <- make.names(names(training_decathlon_set))
RegModel.17 <- lm(X1500m ~ Discus + High.jump + Javeline + Long.jump + Points + Pole.vault + 
    Rank + Shot.put + X100m + X110m.hurdle + X400m, data = training_decathlon_set)
summary(RegModel.17)
```

We can see based on the results that the rank is not affecting the model, so we can remove it.

```{r}
RegModel.18 <- lm(X1500m ~ Discus + High.jump + Javeline + Long.jump + Points + Pole.vault + 
    Shot.put + X100m + X110m.hurdle + X400m, data = training_decathlon_set)
summary(RegModel.18)
```

Here we have a very good model to predict the next results. We have an slope since the **p-value** has a small value **4.842e-14**

Lets try to remove Points to see what happend with the model


```{r}
RegModel.19 <- lm(X1500m ~ Discus + High.jump + Javeline + Long.jump + Pole.vault + 
    Shot.put + X100m + X110m.hurdle + X400m, data = training_decathlon_set)
summary(RegModel.19)
```

We can see that if we remove points, we dont have almost relation between the variables to explain the model. Lets add Points again and remove other Variable to see if the relations stay still.


```{r}
RegModel.21 <- lm(X1500m ~ Discus + High.jump + Long.jump + Points + Pole.vault + 
    Shot.put + X100m + X110m.hurdle + X400m, data = training_decathlon_set)
summary(RegModel.21)
```

We can see again if we keep Points and remove other like Javaline, we again couldnt have a model to do predictions. We are going to add again Javaline.

```{r}
RegModel.23 <- lm(X1500m ~ Discus + High.jump + Javeline + Long.jump + Points + Pole.vault + 
    Shot.put + X100m + X110m.hurdle + X400m, data = training_decathlon_set)
summary(RegModel.23)
```

### Testing assumptions on this linear model

1. Durbin Watson Test for independence

```{r}
library("lmtest", lib.loc = "~/R/win-library/3.0")
dwtest(RegModel.23, alternative = "two.sided")
```

We can see that the Regretion Model pass the independence test whith a p-value of **0.3621**

2. Shapiro Test for Normality 

```{r}
shapiro.test(residuals(RegModel.23))
```

We can see that the distribution is a Normal distribution with a p-value of **0.5407**

2. Breusch Pagan Test - Homogenity of the Variance


```{r}
bptest(RegModel.23)
```

We can see here that since the p-value is small **0.2054**, although not too much, the variance seems homogenous.


### Predictions


```{r}
prediction <- predict(RegModel.23, newdata = test_decathlon_set, interval = "prediction")
prediction
```

Lets compare with the Real data

```{r}
test_only_with_x1500m <- subset(test_decathlon_set, select = c("X1500m"))
test_only_with_x1500m
```

```{r}
predicted_data <- data.frame(prediction)
predicted_data["real_data_x1500m"] = test_only_with_x1500m["X1500m"]
predicted_data
```

As we can appreciate all the real values of the test data set are between the interval **lwr** and **upr**, which indicates that the prediction was properly done, because the model with the trained data was accurate.



